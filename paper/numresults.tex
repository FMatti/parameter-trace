
\section{Numerical experiments}
\label{sec:results}


In this section, we verify the performance of the Chebyshev-Nyström++ algorithm proposed in this paper in application scenarios from electronic structure interaction and neural network optimization.

While theoretically an important tool, we have observed that the preservation of non-negativity discussed in~\cref{sec:preservnonneg} is not needed in practice. The (slight) indefiniteness of the standard Chebyshev approximation $g_{\sigma}^{(m)}(t \mtx{I}_n - \mtx{A})$ from~\cref{equ:matrix-approximation} does not seem to impede the reliability of the Nyström++ estimator. Unless otherwise stated, we transform the input matrices such that their eigenvalues are contained in $[-1, 1]$, by approximating their spectrum with NumPy's Hermitian eigenvalue solver before we apply the methods to them. We compute spectral densities at $n_t = 100$ uniformly distributed values of the parameter $t \in [-1, 1]$. The integral involved in computing the $L^1$-errors is approximated with the composite midpoint quadrature rule using the $n_t$ values of $t$ as nodes. Both parameters introduced at the end of \cref{subsubsec:chebyshev-nystrom-implementation}, the eigenvalue truncation threshold and the parameter for detecting a vanishing spectral density, are fixed to $10^{-5}$.

Our implementations are written in Python 3.12.3 using the packages NumPy 2.0.0 and SciPy 1.14.1. They are executed on a single thread of a GitHub-hosted Ubuntu runner with a 64-bit processor and 16 GB of RAM. 

\subsection{Spectral density for Hamiltonian of electronic structure}
\label{subsec:hamiltonian}

We use the electronic structure interaction example from~\cite[Section 6]{lin-2017-randomized-estimation}, which involves the second order finite difference discretization of the Hamiltonian
\begin{equation}
    \mathcal{H} = - \Delta + V
    \label{equ:electronic-hamiltonian}
\end{equation}
in three dimensions. The potential $V$ interacting with the electrons is generated by Gaussian wells
    $v(r) = v_0 e^{-\lambda r^2}$, 
with $v_0 = -4$ and $\lambda = 8$, centered in cells of side length $L=6$ which are stacked $n_c \in \mathbb{N}$ times in each spatial dimension; see \cref{fig:gaussian-well}. The grid width of the finite difference discretization is fixed to $h=0.6$. For $n_c = 1$, this leads to a sparse matrix of size $1\,000 \times 1\,000$ and for $n_c = 3$ of size $27\,000 \times 27\,000$. This example represents an idealized model for the interaction of nuclei on a regular grid with electrons for a $k$-vector in the center of the first Brillouin zone. The distribution of the eigenvalues of the Hamiltonian --- its spectral density --- allows one to interpret the system's energy levels.

\begin{figure}[ht]
    \begin{subfigure}[b]{0.32\columnwidth}
        \scalebox{0.7}{\input{plots/gaussian-well-1.pgf}}
        \caption{$n_c=1.$}
        \label{fig:gaussian-well-1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \scalebox{0.7}{\input{plots/gaussian-well-2.pgf}}
        \caption{$n_c=2.$}
        \label{fig:gaussian-well-2}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \scalebox{0.7}{\input{plots/gaussian-well-3.pgf}}
        \caption{$n_c=3.$}
        \label{fig:gaussian-well-3}
    \end{subfigure}
    \caption{Cross-sections of the periodic Gaussian wells potential $V$ for different $n_c$.}
    \label{fig:gaussian-well}
\end{figure}

\cref{fig:convergence} displays the convergence behavior of Chebyshev-Nyström++ applied to the matrix $\mtx{A}$ resulting from the discretization of the Hamiltonian for $n_c = 1$ and $n_c = 3$. Unlike \cite{lin-2017-randomized-estimation}, where just a small portion of the spectral density is considered, we approximate the whole spectrum of $\mtx{A}$ (transformed to $[-1, 1]$) in accordance with our theoretical results (\cref{subsubsec:chebyshev-nystrom-analysis}) and to demonstrate the effectiveness of our implementational particularities (\cref{subsubsec:chebyshev-nystrom-implementation}). As a reference, we use the eigenvalues computed by NumPy's Hermitian eigenvalue solver. As expected from \cref{thm:hutchinson}, Girard-Hutchinson alone ($n_{\mtx{\Omega}} = 0$) requires $n_{\mtx{\Psi}} = \mathcal{O}(\varepsilon^{-2})$ samples to achieve an error of order $\varepsilon$. Because the eigenvalues of the matrix are spread out, we observe the better convergence of the Nyström approximation discussed at the end of \cref{sec:application} once $n_{\mtx{\Omega}}$ is sufficiently large. At this point, we can observe that the errors stagnate around $10^{-6}$ as a consequence of the error made in the Chebyshev approximation. Due to the higher eigenvalue density of $\mtx{A}$ for $n_c = 3$, the stronger convergence can be observed to kick in later in this case, because a larger $n_{\mtx{\Omega}}$ is needed for a good Nyström approximation. In fact, for $n_c = 5$ the eigenvalue density is so high that to profit from this effect, $n_{\mtx{\Omega}}$ would need to be chosen so large that the computations become unfeasible.
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \scalebox{0.8}{\input{plots/convergence-1.pgf}}
        \caption{$n_c = 1.$}
        \label{fig:convergence-1}
    \end{subfigure}
    \begin{subfigure}[b]{0.495\textwidth}
        \scalebox{0.8}{\input{plots/convergence-3.pgf}}
        \caption{$n_c = 3.$}
        \label{fig:convergence-3}
    \end{subfigure}
    \caption{Error vs. number of random vectors $n_{\mtx{\Omega}} + n_{\mtx{\Psi}}$ when applying  Chebyshev-Nyström++ to the Hamiltonian matrix from~\cref{subsec:hamiltonian}, with fixed $\sigma=0.005$ and $m=1\,000$, such that the error of the Chebyshev approximation remains sufficiently small according to \cref{fig:chebyshev-heatmap}.}
    \label{fig:convergence}
\end{figure}

For a fixed budget of $n_{\mtx{\Omega}} + n_{\mtx{\Psi}} = 80$ of random vectors, \cref{fig:distribution} shows how the value of the smoothing parameter $\sigma$ impacts the choice between Nyström ($n_{\mtx{\Omega}}$) and 
Girard-Hutchinson ($n_{\mtx{\Psi}}$). As expected, for small $\sigma$ the Nyström approximation alone suffices, but 
this is not an effective choice for large $\sigma$. Choosing $n_{\mtx{\Omega}} = n_{\mtx{\Psi}}$ constitutes a good compromise.


\begin{figure}[ht]
    \centering
    \scalebox{0.8}{ \input{plots/distribution.pgf}}
    \caption{Error vs. $\sigma$ for a fixed budget of $80$ random vectors 
    when applying  Chebyshev-Nyström++ to the Hamiltonian matrix from~\cref{subsec:hamiltonian} with $n_c = 1$.
The Chebyshev approximation error is made negligible by setting $m=16 / \sigma$ (see \cref{lem:non-negative-chebyshev-error}).}
    \label{fig:distribution}
\end{figure}

\subsection{Comparison to Krylov-aware trace estimator}
\label{subsec:krylov-aware}

We notice that the Krylov-aware stochastic trace estimator from \cite[Algorithm 3.1]{chen-2023-krylovaware-stochastic} can also effectively be used in the setting of spectral density approximation. It also samples two Gaussian random matrices $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$ and $\mtx{\Psi} \in \mathbb{R}^{n \times n_{\mtx{\Psi}}}$. It first runs block Lanczos algorithm on $\mtx{A}$ with starting block $\mtx{\Omega}$ for $r$ iterations with reorthogonalization and subsequently $q$ without. The columns of $\mtx{\Psi}$ are then projected onto the complement of the block Krylov subspace generated by the Lanczos algorithm, and are afterwards used as starting vectors for $r$ iterations of the Lanczos algorithm on $\mtx{A}$. What is remarkable is that all these heavy computations are completely independent of the smoothing kernel $g_{\sigma}$ and the parameter $t$. Further, some algebraic manipulations show that similarly to the stochastic Lanczos quadrature \cite[Section 3]{ubaru-2017-fast-estimation}, we can compress the estimator into a quadrature. Since this quadrature is independent of $g_{\sigma}$ and $t$, it can be applied to different smoothing kernels $g_{\sigma}$ and to as many values of $t$ as needed --- at little additional cost.

We run our own, faithfully optimized implementation of the Krylov-aware stochastic trace estimator \cite[Algorithm 3.1]{chen-2023-krylovaware-stochastic} for different parameter settings on the example from \cref{subsec:hamiltonian} and plot the approximation errors for logarithmically spaced values of the smoothing parameter $\sigma$ in \cref{fig:krylov-aware-density}. For reference, we also report the error of Chebyshev-Nyström++ for parameters that lead to a comparable run-time. The non-monotonic behavior of the Chebyshev-Nyström++ method in this case arises because, for small values of $\sigma$, the Chebyshev interpolation requires a very high polynomial degree to be accurate, whereas for large $\sigma$, the low-rank approximation becomes less effective. Due to the fixed parameters, the sweet spot lies around $\sigma \approx 0.002$: the low-rank approximation benefits from some underlying low-rank structure in $g_{\sigma}(t \mtx{I} - \mtx{A})$, while the Chebyshev interpolation remains good because $g_{\sigma}$ is not too sharply peaked. While the Krylov-aware estimator is clearly an attractive alternative, we argue that the parameters of the Chebyshev-Nyström++ method can be chosen in a more interpretable way, due to the explicit requirement for $m$ based on $\sigma$ (\cref{fig:chebyshev-heatmap}) and for $n_{\mtx{\Omega}}$ based on the distribution of the eigenvalues (discussed towards the end of \cref{subsubsec:chebyshev-nystrom-analysis}).

\begin{figure}[ht]
    \begin{minipage}[c]{.515\linewidth}
        \centering
        \scalebox{0.8}{\input{plots/krylov_aware_density.pgf}}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.475\linewidth}
        \vspace{-35pt}
        \scalebox{0.8}{\input{tables/krylov_aware_density_KA.tex}}
        \newline
        \vspace{15pt}
        \newline
        \scalebox{0.8}{\input{tables/krylov_aware_density_CN.tex}}
    \end{minipage}
    \caption{For the example from \cref{subsec:hamiltonian}, error vs. $\sigma$ the Krylov-Aware (KA) stochastic trace estimator with different parameter settings, compared to the Chebyshev-Nyström++ (CN++) method.}
    \label{fig:krylov-aware-density}
\end{figure}

\subsection{Spectral density of Hessian matrix of neural network}
\label{subsec:hessian}

When fitting the parameters of a neural network, it can be of interest to determine whether a (local) minimum has been found and whether this minimum is robust, i.e., whether a small perturbation of the parameters leads to a significant decrease in the fit of the neural network. Both properties are reflected in the eigenvalues of the Hessian matrix $\mtx{A}$ with respect to some loss function: if all eigenvalues are non-negative, then the loss attains a local minimum, and if, additionally, all of them are small, we talk about a flat minimum; one where small perturbations of the parameters do not cause large increases of the loss and are therefore associated with better generalization \cite{yao-2020-pyhessian-neural}. On the other hand, large eigenvalues in the spectrum indicate high sensitivity of the loss to small changes in the parameters. Those we want to avoid, which is the reason why we monitor them.

Since neural networks are usually parametrized by a large number of parameters, assembling the Hessian matrix explicitly and then computing its eigenvalues is clearly infeasible. On the other hand, one can cheaply compute matrix-vector products $\mtx{A} \vct{x}$ with the Hessian, at a cost that scales proportionally with the number of parameters \cite{pearlmutter-1994-fast-exact}.

To demonstrate that our algorithm can effectively be applied in the setting of neural network optimization, we approximate the spectral density of a small fully connected convolutional neural network with $6\,782$ parameters. We train this network on the handwritten digit classification task given by the MNIST dataset\footnote{Handwritten digit classification; taken from \url{http://yann.lecun.com/exdb/mnist}.} in PyTorch 2.4.1 in a standard way. We use the bound from \cite[Theorem 1]{zhou-2011-bounding-spectrum} to estimate bounds on the spectrum.

We plot the spectral density of the Hessian matrix of the untrained neural network, and at different stages of training in \cref{fig:hessian-density}, as well as the corresponding mean squared error loss. It can be observed that the eigenvalues creep towards zero as the training proceeds. Furthermore, despite the loss steadily decreasing, the presence of relatively large eigenvalues in some epochs indicates a sharp minimum of the network, hence, unfavorable generalization properties.

\begin{figure}
    \begin{minipage}[c]{.49\linewidth}
        \centering
        \scalebox{0.8}{\input{plots/hessian_density.pgf}}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.49\linewidth}
        \centering
        \scalebox{0.8}{\input{plots/hessian_density_loss.pgf}}
    \end{minipage}
    \caption{Mean squared error training loss (right) and corresponding approximate spectral density of the Hessian matrix of a fully connected convolutional neural network in different epochs of training on the MNIST dataset (left). The spectral density is approximated in $n_t=150$ uniformly spaced points using Chebyshev-Nyström++ with parameters $n_{\mtx{\Omega}} = 10$, $n_{\mtx{\Psi}} = 10$, $m = 1000$, and $\sigma = 0.005$.}
    \label{fig:hessian-density}
\end{figure}
