
\section{Numerical experiments}
\label{sec:results}

\color{black}

In this last section, we test the Chebyshev-Nyström++ algorithm in multiple scenarios leading from electronic structure interaction, statistical thermodynamics, and neural network optimization.

While theoretically an important tool, for the remainder of this section, we waive using the non-negative Chebyshev approximation, since the (slight) indefiniteness of $g_{\sigma}^{(m)}(t \mtx{I}_n - \mtx{A})$ \refequ{equ:matrix-approximation} does not seem to matter when computing the Nyström++ estimator in finite precision arithmetic. Unless otherwise mentioned, we transform the matrices such that all their eigenvalues are contained in $[-1, 1]$ by approximating the spectrum with NumPy's Hessian eigenvalue solver. We compute the spectral densities at $n_t = 100$ uniformly distributed values of the parameter $t$ in $[-1, 1]$. The integral involved in computing the $L^1$-errors is approximated with the composite midpoint quadrature rule using the $n_t$ values of $t$ as nodes. Both parameters introduced at the end \refsec{subsubsec:chebyshev-nystrom-implementation},the eigenvalue truncation threshold and the parameter for detecting a vanishing spectral density, are both set to $10^{-5}$.

All our implementations are written in Python 3.12.3 using the packages NumPy 2.0.0 and SciPy 1.14.1. They are executed on a single thread of a GitHub-hosted Ubuntu runner with a 64-bit processor and 16 GB of RAM. 

\subsection{Spectral density for Hamiltonian of electronic structure}
\label{subsec:hamiltonian}

Our first numerical example comes from electronic structure interaction \cite[Section 6]{lin-2017-randomized-estimation}, more precisely from the second order finite difference discretization of the Hamiltonian
\begin{equation}
    \mathcal{H} = - \Delta + V
    \label{equ:5-experiments-electronic-hamiltonian}
\end{equation}
in three dimensions. The potential $V$ interacting with the electrons is generated by Gaussian wells
\begin{equation}
    v(r) = v_0 e^{-\lambda r^2}
    \label{equ:5-experiments-gaussian-cell}
\end{equation}
with $v_0 = -4$ and $\lambda = 8$, centered in cells of side length $L=6$ which are stacked $n_c \in \mathbb{N}$ times in each spatial dimension (cf. \reffig{fig:gaussian-well}). The grid width is fixed to be $h=0.6$. For example, for $n_c = 1$, this leads to a matrix of size $1000 \times 1000$. This is an idealized model for the interaction of nuclei on a regular grid with electrons for a $k$-vector in the center of the first Brillouin zone. The distribution of the eigenvalues of the Hamiltonian --- its spectral density --- allows us to interpret the system's energy levels.

\begin{figure}[ht]
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/gaussian-well-1.pgf}
        \caption{$n_c=1$}
        \label{fig:gaussian-well-1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/gaussian-well-2.pgf}
        \caption{$n_c=2$}
        \label{fig:gaussian-well-2}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/gaussian-well-5.pgf}
        \caption{$n_c=5$}
        \label{fig:gaussian-well-5}
    \end{subfigure}
    \caption{Cross-sections of the periodic Gaussian well potential $V$ for different sizes $n_c$ of the supercell.}
    \label{fig:gaussian-well}
\end{figure}

For a fixed total number of random vectors $n_{\mtx{\Psi}} + n_{\mtx{\Omega}}$, we analyze the convergence behavior of the Chebyshev-Nyström++ method when run on the $1000 \times 1000$ matrix resulting from the finite difference discretization of the Hamiltonian for $n_c = 1$ described above (see \reffig{fig:convergence}). As a reference, we use the eigenvalues computed by NumPy's Hermitian eigenvalue solver applied to the matrix $\mtx{A}$.

\begin{figure}[ht]
    \centering
    \input{plots/convergence.pgf}
    \caption{For increasing values of $n_{\mtx{\Psi}} + n_{\mtx{\Omega}}$ but fixed $m$, we apply the Chebyshev-Nyström++ method to the Hamiltonian matrix described in \refsec{subsec:hamiltonian}. We plot the $L^1$-approximation error for $\sigma=0.005$ on the model problem with $n_c = 1$.}
    \label{fig:convergence}
\end{figure}

Indeed, we observe that for $n_{\mtx{\Omega}} = 0$, we require $n_{\mtx{\Psi}} = \mathcal{O}(\varepsilon^{-2})$ to achieve an error of order $\varepsilon$, as shown in \refthm{thm:hutchinson}. Because the eigenvalues of the matrix are quite uniformly distributed, we observe the stronger convergence of the Nyström approximation discussed in the end of \refsec{sec:application} once $n_{\mtx{\Omega}}$ is large enough.

For a fixed budget of random vectors $n_{\mtx{\Omega}} + n_{\mtx{\Psi}}$, \reffig{fig:distribution} visualizes how for different values of the smoothing parameter $\sigma$ the random vectors should either be invested into the Nyström approximation $n_{\mtx{\Omega}}$ or the estimation of the residual trace $n_{\mtx{\Psi}}$. Again, since for small $\sigma$ the matrix function $g_{\sigma}^{(m)}(t \mtx{I}_n - \mtx{A})$ has a small numerical rank, a Nyström approximation alone is already enough to achieve a good approximation. On the other hand, when $\sigma$ is large, the Nyström approximation by itself is not effective, and the correction with the residual trace becomes indispensable.

\begin{figure}[ht]
    \centering
    \input{plots/distribution.pgf}
    \caption{For a range of values of $\sigma$, the Chebyshev-Nyström++ method is applied to the Hamiltonian matrix described in \refsec{subsec:hamiltonian} with $n_c = 1$ for different ways of allocating a total of $n_{\mtx{\Psi}} + n_{\mtx{\Omega}}=80$ random vectors to either the Nystr\"om low-rank approximation or the Girard-Hutchinson trace estimation. We make the approximation error committed by the Chebyshev approximation negligible by rescaling $m=16 / \sigma$ (based on \reflem{lem:non-negative-chebyshev-error}).}
    \label{fig:distribution}
\end{figure}

\subsection{Comparison to Krylov-aware stochastic trace estimation}
\label{subsec:krylov-aware}

The Krylov-aware stochastic trace estimator \cite[Algorithm 3.1]{chen-2023-krylovaware-stochastic} is also suitable in the context of spectral density estimation. It also samples two Gaussian random matrices $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\mtx{\Omega}}}$ and $\mtx{\Psi} \in \mathbb{R}^{n \times n_{\mtx{\Psi}}}$. $\mtx{\Omega}$ is used to extract a low-rank approximation to $g_{\sigma}(t \mtx{I}_n - \mtx{A})$ by running the block-Lanczos algorithm on $\mtx{A}$ with starting block $\mtx{\Omega}$ for $r$ iterations with reorthogonalization and subsequently $q$ without. The columns of $\mtx{\Psi}$ are then used to estimate the trace of the approximation residual with $r$ iterations of the Lanczos algorithm on $\mtx{A}$.

We run our own, faithfully optimized implementation of the Krylov-aware stochastic trace estimator \cite[Algorithm 3.1]{chen-2023-krylovaware-stochastic} in multiple configurations on the example from \refsec{subsec:hamiltonian} and plot the approximation errors for logarithmically spaced values of the smoothing parameter $\sigma$ in \reffig{fig:krylov-aware-density}. For reference, we add the error of the Chebyshev-Nyström++ method on parameters which lead to a comparable run-time.

\begin{figure}[ht]
    \begin{minipage}[c]{.475\linewidth}
        \centering
        \input{plots/krylov_aware_density.pgf}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.475\linewidth}
        \vspace{-35pt}
        \input{tables/krylov_aware_density_KA.tex}
        \newline
        \vspace{15pt}
        \newline
        \input{tables/krylov_aware_density_CN.tex}
    \end{minipage}
    \caption{For the example from \refsec{subsec:hamiltonian} we compare the Chebyshev-Nyström++ (CN++) method with the Krylov-Aware (KA) stochastic trace estimator in multiple configurations. To this extent, we compute the $L^1$-error of the methods for various levels of smoothing $\sigma$ (left). The parameters of the algorithms and run-time averaged over all values of $\sigma$ can be found in the tables (right).}
    \label{fig:krylov-aware-density}
\end{figure}

\subsection{Spectral density of Hessian matrix of neural network}
\label{subsec:hessian}

When optimizing a neural network over some data, it is of great interest to determine whether a minimum has been found and if this minimum is robust, i.e. a small perturbation of the parameters of the neural network will not lead to a significant decrease in the fit of the neural network. Both properties are reflected in the eigenvalues of the Hessian matrix $\mtx{A}$ with respect to some loss function: if all eigenvalues are non-negative, then the loss attains a local minimum, and if additionally all of them are small, the minimum is robust.

Since neural networks are usually parametrized by thousands of parameters, assembling the Hessian matrix explicitly and then computing its eigenvalues is out of question. Luckily, there exists a method which can exactly compute matrix-vector products $\mtx{A} \vct{x}$ for any vector $\vct{x}$ which scales proportionally with the number of parameters \cite{pearlmutter-1994-fast-exact}.

To demonstrate that our algorithm can effectively be applied in the setting of neural network optimization, we approximate the spectral density of a small fully connected convolutional neural network with 6782 parameters. We train this network on the handwritten digit classification task given by the MNIST dataset\footnote{Handwritten digit classification; taken from \url{http://yann.lecun.com/exdb/mnist}.} in PyTorch 2.4.1 in a standard way. We use the bound from \cite[Theorem 1]{zhou-2011-bounding-spectrum} to estimate a bound on the spectrum.

We plot the approximation of the spectral density of the Hessian matrix of the untrained neural network, and at different stages of training in \reffig{fig:hessian-density}, as well as the corresponding mean squared error loss. It can well be observed that the eigenvalues creep towards zero as the training proceeds. Furthermore, despite the loss steadily decreasing, the presence of relatively large eigenvalues in some epochs indicates a sharp minimum of the network, hence, unfavorable generalization properties.

\begin{figure}
    \begin{minipage}[c]{.49\linewidth}
        \centering
        \input{plots/hessian_density.pgf}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.49\linewidth}
        \centering
        \input{plots/hessian_density_loss.pgf}
    \end{minipage}
    \caption{The mean squared error training loss (right) and the corresponding approximate spectral density of the Hessian matrix of a fully connected convolutional neural network in different epochs of training on the MNIST dataset (left). The spectral density is approximated in $n_t=150$ uniformly spaced points using the Chebyshev-Nyström++ method (\refalg{alg:nystrom-chebyshev-pp}) with parameters $n_{\mtx{\Omega}} = 10$, $n_{\mtx{\Psi}} = 10$, $m = 1000$, and $\sigma = 0.005$.}
    \label{fig:hessian-density}
\end{figure}

\subsection{Approximation of the partition function of a quantum system}
\label{subsec:spin-chain}

To demonstrate that the Chebyshev-Nyström++ estimator is also effective on problems unrelated to spectral density estimation, we consider the problem of approximating the partition function $\Trace(\exp(-\beta \mtx{A}))$ for the Hamiltonian $\mtx{A}$ of a planar Heisenberg spin chain \cite[Section 5.1]{chen-2023-krylovaware-stochastic}. We compute the pointwise error from the analytic solution as a function of the temperature $\beta^{-1}$ and compare it to the error achieved by the Krylov-aware trace estimator in \reffig{fig:krylov-aware-spin}.

\begin{figure}[ht]
    \begin{minipage}[c]{.475\linewidth}
        \centering
        \input{plots/krylov_aware_spin.pgf}
    \end{minipage}\hfill%
    \begin{minipage}[c]{.475\linewidth}
        \vspace{-35pt}
        \input{tables/krylov_aware_spin_KA.tex}
        \newline
        \vspace{15pt}
        \newline
        \input{tables/krylov_aware_spin_CN.tex}
    \end{minipage}
    \caption{For the Heisenberg spin chain example, we compare the Chebyshev-Nyström++ (CN++) method with the Krylov-Aware stochastic trace estimator in the same configurations as in \cite[Table 5.1]{chen-2023-krylovaware-stochastic} (right). To this extent, we compute the $L^1$-error of the methods for various choices of the temperature parameter $\beta^{-1}$ (left).}
    \label{fig:krylov-aware-spin}
\end{figure}

Unlike in the spectral density example from \refsec{subsec:hamiltonian}, where the numerical rank of the matrix stays low for all $t$, it changes wildly with the parameter $\beta$. The Chebyshev-Nyström++ method requires a fixed allocation of random vectors to the low-rank approximation for all values of the parameter $\beta$. Hence, both for large $\beta$, where the matrix $\exp(-\beta \mtx{A})$ is low-rank, and for small $\beta$, where the singular values only decay slowly, we cannot expect a good approximation.
